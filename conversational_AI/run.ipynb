{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf868e51-a011-40d8-8f87-0ce0e0ecd8c0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simpletransformers in /usr/local/lib/python3.9/dist-packages (0.63.7)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.8.1)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2.3.2)\n",
      "Requirement already satisfied: seqeval in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.2.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (4.64.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.1.1)\n",
      "Requirement already satisfied: wandb>=0.10.32 in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (0.13.1)\n",
      "Requirement already satisfied: tokenizers in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (0.12.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.23.1)\n",
      "Requirement already satisfied: streamlit in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.12.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2.9.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (0.1.96)\n",
      "Requirement already satisfied: transformers>=4.6.0 in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (4.20.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (1.4.3)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from simpletransformers) (2022.7.9)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (0.8.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers>=4.6.0->simpletransformers) (3.7.1)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (1.0.9)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (0.4.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (5.9.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (63.1.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (1.9.4)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (3.1.27)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (3.19.4)\n",
      "Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb>=0.10.32->simpletransformers) (1.14.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (1.3.2)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (2.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (8.1.3)\n",
      "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb>=0.10.32->simpletransformers) (0.1.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->simpletransformers) (2.8)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->simpletransformers) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->simpletransformers) (2019.11.28)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (2022.5.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (0.3.5.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (3.8.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (8.0.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets->simpletransformers) (0.70.13)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->simpletransformers) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->simpletransformers) (2022.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->simpletransformers) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->simpletransformers) (1.1.0)\n",
      "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (5.2.0)\n",
      "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (4.2.0)\n",
      "Requirement already satisfied: pympler>=0.9 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (1.0.1)\n",
      "Requirement already satisfied: tzlocal>=1.1 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (4.2)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (4.3.0)\n",
      "Requirement already satisfied: pydeck>=0.1.dev5 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (0.7.1)\n",
      "Requirement already satisfied: semver in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (2.13.0)\n",
      "Requirement already satisfied: watchdog in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (2.1.9)\n",
      "Requirement already satisfied: validators>=0.2 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (0.20.0)\n",
      "Requirement already satisfied: blinker>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (1.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (9.2.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (12.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (4.12.0)\n",
      "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.9/dist-packages (from streamlit->simpletransformers) (6.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (1.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (0.4.6)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (0.35.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (2.9.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (2.1.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (1.47.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard->simpletransformers) (3.3.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.1.2)\n",
      "Requirement already satisfied: toolz in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.12.0)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.9/dist-packages (from altair>=3.2.0->streamlit->simpletransformers) (4.7.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (4.0.9)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (1.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=1.4->streamlit->simpletransformers) (3.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=20.0->transformers>=4.6.0->simpletransformers) (3.0.9)\n",
      "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.9/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.7.1)\n",
      "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.9/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.3.0)\n",
      "Requirement already satisfied: ipykernel>=5.1.2 in /usr/local/lib/python3.9/dist-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (6.15.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit->simpletransformers) (2.12.0)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.9/dist-packages (from rich>=10.11.0->streamlit->simpletransformers) (0.9.1)\n",
      "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.9/dist-packages (from tzlocal>=1.1->streamlit->simpletransformers) (0.1.0.post0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from validators>=0.2->streamlit->simpletransformers) (5.1.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets->simpletransformers) (6.0.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb>=0.10.32->simpletransformers) (5.0.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (7.3.4)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.3)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.5)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (8.4.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (23.2.0)\n",
      "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.6.2)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.6.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (2.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit->simpletransformers) (0.18.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->simpletransformers) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->simpletransformers) (3.2.0)\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.9/dist-packages (from pytz-deprecation-shim->tzlocal>=1.1->streamlit->simpletransformers) (2022.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.18.1)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.0.30)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.3.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\n",
      "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.9/dist-packages (from jupyter-client>=6.1.12->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.11.1)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (6.4.12)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.3)\n",
      "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (6.5.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.9.0)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.8.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.15.0)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (21.3.0)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.4.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.2)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.1)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.6)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.11.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.1.1)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.15.3)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.15.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.3.2.post1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.21)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: fugashi[unidic-lite] in /usr/local/lib/python3.9/dist-packages (1.1.2)\n",
      "Requirement already satisfied: unidic-lite in /usr/local/lib/python3.9/dist-packages (from fugashi[unidic-lite]) (1.0.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install simpletransformers\n",
    "!pip install fugashi[unidic-lite]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35e0e643-9b16-454b-a373-e40dbe9d143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from transformers import T5Tokenizer, GPT2LMHeadModel, GPT2Config, AutoModelForCausalLM, GPT2DoubleHeadsModel\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup, LambdaLR\n",
    "import pandas as pd\n",
    "from itertools import chain\n",
    "import os\n",
    "from tqdm.auto import tqdm, trange\n",
    "from simpletransformers.config.model_args import ConvAIArgs\n",
    "from simpletransformers.conv_ai.conv_ai_utils import get_dataset\n",
    "from simpletransformers.config.utils import sweep_config_to_sweep_values\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from sklearn.metrics import (\n",
    "    f1_score\n",
    ")\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e50734-15c5-4190-beb2-1a0720867a06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ConvAIModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13a3132d-f047-4429-97ae-ff0d3b1c9249",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "SPECIAL_TOKENS = [\"<s>\", \"</s>\", \"<speaker1>\", \"<speaker2>\", \"[PAD]\"]\n",
    "ATTR_TO_SPECIAL_TOKEN = {\n",
    "    \"bos_token\": \"<s>\",\n",
    "    \"eos_token\": \"</s>\",\n",
    "    \"pad_token\": \"<PAD>\",\n",
    "    \"additional_special_tokens\": [\"<speaker1>\", \"<speaker2>\"],\n",
    "}\n",
    "MODEL_INPUTS = [\"input_ids\", \"mc_token_ids\", \"labels\", \"mc_labels\", \"token_type_ids\"]\n",
    "PADDED_INPUTS = [\"input_ids\", \"labels\", \"token_type_ids\"]\n",
    "class ConvAIModel: # modified load_tokenizer = False\n",
    "    def __init__(self, model_name, load_tokenizer=False, args=None, **kwargs):\n",
    "        self.args = self._load_model_args(model_name)\n",
    "        if \"sweep_config\" in kwargs:\n",
    "            self.is_sweeping = True\n",
    "            sweep_config = kwargs.pop(\"sweep_config\")\n",
    "            sweep_values = sweep_config_to_sweep_values(sweep_config)\n",
    "            self.args.update_from_dict(sweep_values)\n",
    "        else:\n",
    "            self.is_sweeping = False\n",
    "        if isinstance(args, dict):\n",
    "            self.args.update_from_dict(args)\n",
    "        elif isinstance(args, ConvAIArgs):\n",
    "            self.args = args\n",
    "            \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        # 日本語版のモデルに合わせる\n",
    "        # modeified \n",
    "        if load_tokenizer:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(self.args.tokenizer_dir)\n",
    "        else:\n",
    "            self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        #\n",
    "        self.tokenizer.do_lower_case = True\n",
    "        # AutoModelForCausalLM\n",
    "        self.model = GPT2DoubleHeadsModel.from_pretrained(model_name)\n",
    "        self.config = GPT2Config.from_pretrained(model_name)\n",
    "        self.add_special_tokens_(self.model, self.tokenizer)\n",
    "    def train_model(self, train_file=None, output_dir=None, show_running_loss=True, args=None, eval_file=None, verbose=True, **kwargs):\n",
    "        if self.args.evaluate_during_training and eval_file is None:\n",
    "            warnings.warn(\n",
    "                \"eval_file not specified but evaluate_during_training is True. Using personachat eval data.\"\n",
    "            )\n",
    "        if args:\n",
    "            self.args.update_from_dict(args)\n",
    "        if not output_dir:\n",
    "            output_dir = self.args.output_dir\n",
    "        if (\n",
    "            os.path.exists(output_dir)\n",
    "            and os.listdir(output_dir)\n",
    "            and not self.args.overwrite_output_dir\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"Output directory ({}) already exists and is not empty.\"\n",
    "                \" Set overwrite_output_dir: True to automatically overwrite.\".format(\n",
    "                    output_dir\n",
    "                )\n",
    "            )\n",
    "        self._move_model_to_device()\n",
    "        wandb.init()\n",
    "        train_dataloader, train_sampler = self.load_and_cache_examples(\n",
    "            dataset_path=train_file,\n",
    "            verbose=verbose,\n",
    "            no_cache=self.args.no_cache or self.args.reprocess_input_data,\n",
    "        )\n",
    "        eval_loader = None\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        global_step, training_details = self.train(\n",
    "            train_dataloader,\n",
    "            output_dir,\n",
    "            show_running_loss=show_running_loss,\n",
    "            verbose=verbose,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.save_model(model=self.model)\n",
    "        wandb.save(self.args.wandb_dir)\n",
    "        if verbose:\n",
    "            logger.info(\n",
    "                \" Training of {} model complete. Saved to {}.\".format(\n",
    "                    self.args.model_type, output_dir\n",
    "                )\n",
    "            )\n",
    "    def train(self, train_dataloader, output_dir, show_running_loss=True, verbose=True, **kwargs,):\n",
    "        device = self.device\n",
    "        model = self.model\n",
    "        args = self.args\n",
    "        if self.args.writer:\n",
    "            tb_writer = SummaryWriter(log_dir=args.tensorboard_dir)\n",
    "        t_total = (\n",
    "            len(train_dataloader)\n",
    "            // args.gradient_accumulation_steps\n",
    "            * args.num_train_epochs\n",
    "        )\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = []\n",
    "        custom_parameter_names = set()\n",
    "        for group in self.args.custom_parameter_groups:\n",
    "            params = group.pop(\"params\")\n",
    "            custom_parameter_names.update(params)\n",
    "            param_group = {**group}\n",
    "            param_group[\"params\"] = [\n",
    "                p for n, p in model.named_parameters() if n in params\n",
    "            ]\n",
    "            optimizer_grouped_parameters.append(param_group)\n",
    "        for group in self.args.custom_layer_parameters:\n",
    "            layer_number = group.pop(\"layer\")\n",
    "            layer = f\"layer.{layer_number}.\"\n",
    "            group_d = {**group}\n",
    "            group_nd = {**group}\n",
    "            group_nd[\"weight_decay\"] = 0.0\n",
    "            params_d = []\n",
    "            params_nd = []\n",
    "            for n, p in model.named_parameters():\n",
    "                if n not in custom_parameter_names and layer in n:\n",
    "                    if any(nd in n for nd in no_decay):\n",
    "                        params_nd.append(p)\n",
    "                    else:\n",
    "                        params_d.append(p)\n",
    "                    custom_parameter_names.add(n)\n",
    "            group_d[\"params\"] = params_d\n",
    "            group_nd[\"params\"] = params_nd\n",
    "            optimizer_grouped_parameters.append(group_d)\n",
    "            optimizer_grouped_parameters.append(group_nd)\n",
    "        if not self.args.train_custom_parameters_only:\n",
    "            optimizer_grouped_parameters.extend(\n",
    "                [\n",
    "                    {\n",
    "                        \"params\": [\n",
    "                            p\n",
    "                            for n, p in model.named_parameters()\n",
    "                            if n not in custom_parameter_names\n",
    "                            and not any(nd in n for nd in no_decay)\n",
    "                        ],\n",
    "                        \"weight_decay\": args.weight_decay,\n",
    "                    },\n",
    "                    {\n",
    "                        \"params\": [\n",
    "                            p\n",
    "                            for n, p in model.named_parameters()\n",
    "                            if n not in custom_parameter_names\n",
    "                            and any(nd in n for nd in no_decay)\n",
    "                        ],\n",
    "                        \"weight_decay\": 0.0,\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        warmup_steps = math.ceil(t_total * args.warmup_ratio)\n",
    "        args.warmup_steps = (\n",
    "            warmup_steps if args.warmup_steps == 0 else args.warmup_steps\n",
    "        )\n",
    "        optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=args.learning_rate,\n",
    "            eps=args.adam_epsilon,\n",
    "        )\n",
    "        # modified\n",
    "        if self.args.Lambda_LR:\n",
    "            scheduler = LambdaLR(optimizer, lr_lambda = lambda epoch: 0.90 ** epoch)\n",
    "        else:\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=args.warmup_steps,\n",
    "                num_training_steps=t_total,\n",
    "            )\n",
    "        if args.n_gpu > 1:\n",
    "            model = torch.nn.DataParallel(model)\n",
    "        global_step = 0\n",
    "        training_progress_scores = None\n",
    "        tr_loss, logging_loss = 0.0, 0.0\n",
    "        model.zero_grad()\n",
    "        train_iterator = trange(\n",
    "            int(args.num_train_epochs), desc=\"Epoch\", disable=args.silent\n",
    "        )\n",
    "        epoch_number = 0\n",
    "        best_eval_metric = None\n",
    "        early_stopping_counter = 0\n",
    "        if args.fp16:\n",
    "            from torch.cuda import amp\n",
    "            scaler = amp.GradScaler()\n",
    "        \n",
    "        for _ in train_iterator:\n",
    "            model.train()\n",
    "            train_iterator.set_description(\n",
    "                f\"Epoch {epoch_number} of {args.num_train_epochs}\"\n",
    "            )\n",
    "            print(' ', datetime.datetime.now())\n",
    "            batch_iterator = tqdm(\n",
    "                train_dataloader,\n",
    "                desc=f\"Running Epoch {epoch_number + 1} of {args.num_train_epochs}\",\n",
    "                disable=args.silent,\n",
    "                mininterval=0,\n",
    "            )\n",
    "            for step, batch in enumerate(batch_iterator):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, mc_token_ids, labels, mc_labels, token_type_ids = batch\n",
    "                if args.fp16:\n",
    "                    with amp.autocast():\n",
    "                        outputs = model(\n",
    "                            input_ids,\n",
    "                            token_type_ids=token_type_ids,\n",
    "                            mc_token_ids=mc_token_ids,\n",
    "                            mc_labels=mc_labels,\n",
    "                            labels=labels,\n",
    "                        )\n",
    "                        lm_loss, mc_loss = outputs[:2]\n",
    "                        # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "                        loss = lm_loss * args.lm_coef + mc_loss * args.mc_coef\n",
    "                else:\n",
    "                    outputs = model(\n",
    "                        input_ids,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        mc_token_ids=mc_token_ids,\n",
    "                        mc_labels=mc_labels,\n",
    "                        labels=labels,\n",
    "                    )\n",
    "                    lm_loss, mc_loss = outputs[:2]\n",
    "                    # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "                    loss = lm_loss * args.lm_coef + mc_loss * args.mc_coef\n",
    "                \n",
    "                if args.n_gpu > 1:\n",
    "                    loss = (\n",
    "                        loss.mean()\n",
    "                    ) \n",
    "                current_loss = loss.item()\n",
    "                if show_running_loss: \n",
    "                    print(\"\\rRunning loss: %f\" % current_loss, end=\"\")\n",
    "                if args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / args.gradient_accumulation_steps\n",
    "                if args.fp16:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                \n",
    "                tr_loss += loss.item()\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    if args.fp16:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                    if args.optimizer == \"AdamW\":\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            model.parameters(), args.max_grad_norm\n",
    "                        )\n",
    "                    if args.fp16:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        optimizer.step()\n",
    "                    scheduler.step()  # Update learning rate schedule\n",
    "                    model.zero_grad()\n",
    "                    global_step += 1\n",
    "                    # necessary\n",
    "                    if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                        if self.args.writer:\n",
    "                            tb_writer.add_scalar(\n",
    "                                \"lr\", scheduler.get_last_lr()[0], global_step\n",
    "                            )\n",
    "                            tb_writer.add_scalar(\n",
    "                                \"loss\",\n",
    "                                (tr_loss - logging_loss) / args.logging_steps,\n",
    "                                global_step,\n",
    "                            )\n",
    "                        logging_loss = tr_loss\n",
    "                        # necessary\n",
    "                        if args.wandb_project or self.is_sweeping:\n",
    "                            wandb.log(\n",
    "                                {\n",
    "                                    \"Training loss\": current_loss,\n",
    "                                    \"lr\": scheduler.get_last_lr()[0],\n",
    "                                    \"global_step\": global_step,\n",
    "                                }\n",
    "                            )\n",
    "                    # not necessary\n",
    "                    if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                        # Save model checkpoint\n",
    "                        output_dir_current = os.path.join(\n",
    "                            output_dir, \"checkpoint-{}\".format(global_step)\n",
    "                        )\n",
    "                        self.save_model(output_dir_current, model=model)\n",
    "            epoch_number += 1\n",
    "            # not necessary\n",
    "            output_dir_current = os.path.join(\n",
    "                output_dir, \"checkpoint-{}-epoch-{}\".format(global_step, epoch_number)\n",
    "            )\n",
    "            # not necessary\n",
    "            if args.save_model_every_epoch or args.evaluate_during_training:\n",
    "                os.makedirs(output_dir_current, exist_ok=True)\n",
    "            # modified\n",
    "            if args.save_model_every_epoch:\n",
    "                self.save_model(output_dir,model=model)\n",
    "                # self.save_model(output_dir_current, model=model)\n",
    "        return (\n",
    "            global_step,\n",
    "            tr_loss / global_step\n",
    "            if not self.args.evaluate_during_training\n",
    "            else training_progress_scores,\n",
    "        )\n",
    "    def load_and_cache_examples(self, dataset_path=None, evaluate=False, no_cache=False, verbose=True, silent=False):\n",
    "        process_count = self.args.process_count\n",
    "        tokenizer = self.tokenizer\n",
    "        args = self.args\n",
    "        if not no_cache:\n",
    "            no_cache = args.no_cache\n",
    "        os.makedirs(self.args.cache_dir, exist_ok=True)\n",
    "        dataset_path = dataset_path if dataset_path else \"\"\n",
    "        dataset = get_dataset(\n",
    "            tokenizer,\n",
    "            dataset_path,\n",
    "            args.cache_dir,\n",
    "            process_count=process_count,\n",
    "            proxies=self.__dict__.get(\"proxies\", None),\n",
    "            evaluate=evaluate,\n",
    "            no_cache=no_cache,\n",
    "            args=args,\n",
    "        )\n",
    "        datasets = defaultdict(list)\n",
    "        num_candidates = len(dataset[0][\"utterances\"][0][\"candidates\"])\n",
    "        if args.num_candidates > 0 and not evaluate:\n",
    "            num_candidates = min(args.num_candidates, num_candidates)\n",
    "        for dialog in dataset:\n",
    "            persona = dialog[\"personality\"].copy()\n",
    "            for _ in range(args.personality_permutations):\n",
    "                for utterance in dialog[\"utterances\"]:\n",
    "                    history = utterance[\"history\"][-(2 * args.max_history + 1) :]\n",
    "                    for j, candidate in enumerate(\n",
    "                        utterance[\"candidates\"][-num_candidates:]\n",
    "                    ):\n",
    "                        labels = bool(j == num_candidates - 1)\n",
    "                        instance = self.build_input_from_segments(\n",
    "                            persona, history, candidate, tokenizer, labels\n",
    "                        )\n",
    "                        for input_name, input_array in instance.items():\n",
    "                            datasets[input_name].append(input_array)\n",
    "                    datasets[\"mc_labels\"].append(num_candidates - 1)\n",
    "                    datasets[\"n_candidates\"] = num_candidates\n",
    "                persona = [persona[-1]] + persona[:-1]  # permuted personalities\n",
    "        \n",
    "        tensor_datasets = []\n",
    "        dataset = self.pad_dataset(\n",
    "            datasets, padding=tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS[-1])\n",
    "        )\n",
    "        for input_name in MODEL_INPUTS:\n",
    "            tensor = torch.tensor(dataset[input_name])\n",
    "            if input_name != \"mc_labels\":\n",
    "                tensor = tensor.view((-1, datasets[\"n_candidates\"]) + tensor.shape[1:])\n",
    "            tensor_datasets.append(tensor)\n",
    "        \n",
    "        tensor_dataset = TensorDataset(*tensor_datasets)\n",
    "        if not evaluate:\n",
    "            data_sampler = RandomSampler(tensor_dataset)\n",
    "            data_loader = DataLoader(\n",
    "                tensor_dataset, sampler=data_sampler, batch_size=args.train_batch_size\n",
    "            )\n",
    "        else:\n",
    "            data_sampler = SequentialSampler(tensor_dataset)\n",
    "            data_loader = DataLoader(\n",
    "                tensor_dataset, sampler=data_sampler, batch_size=args.eval_batch_size\n",
    "            )\n",
    "        return data_loader, data_sampler\n",
    "    def compute_metrics(self, mc_preds, mc_labels, lm_logits, labels, **kwargs):\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        extra_metrics = {}\n",
    "        for metric, func in kwargs.items():\n",
    "            extra_metrics[metric] = func(mc_labels, mc_preds)\n",
    "        f1_current = f1_score(mc_labels.cpu().numpy(), mc_preds, average=\"macro\")\n",
    "        lm_loss_current = loss_fct(lm_logits, labels)\n",
    "        return {\n",
    "            **{\"f1_score\": f1_current, \"language_model_loss\": lm_loss_current},\n",
    "            **extra_metrics,\n",
    "        }\n",
    "    \n",
    "    def interact(self, personality=None):\n",
    "        model = self.model\n",
    "        args = self.args\n",
    "        tokenizer = self.tokenizer\n",
    "        process_count = self.args.process_count\n",
    "        if self.args.fp16:\n",
    "            from torch.cuda import amp\n",
    "        self._move_model_to_device()\n",
    "        if not personality:\n",
    "            dataset = get_dataset(\n",
    "                tokenizer,\n",
    "                None,\n",
    "                args.cache_dir,\n",
    "                process_count=process_count,\n",
    "                proxies=self.__dict__.get(\"proxies\", None),\n",
    "                interact=True,\n",
    "                args=args,\n",
    "            )\n",
    "            personalities = [\n",
    "                dialog[\"personality\"]\n",
    "                for dataset in dataset.values()\n",
    "                for dialog in dataset\n",
    "            ]\n",
    "            personality = random.choice(personalities)\n",
    "        else:\n",
    "            personality = [tokenizer.encode(s.lower()) for s in personality]\n",
    "        \n",
    "        history = []\n",
    "        while True:\n",
    "            raw_text = input(\">>> \")\n",
    "            while not raw_text:\n",
    "                print(\"Prompt should not be empty!\")\n",
    "                raw_text = input(\">>> \")\n",
    "            history.append(\n",
    "                tokenizer.encode(raw_text)\n",
    "            )\n",
    "            with torch.no_grad():\n",
    "                if args.fp16:\n",
    "                    with amp.autocast():\n",
    "                        out_ids = self.sample_sequence(\n",
    "                            personality, history, tokenizer, model, args\n",
    "                        )\n",
    "                else:\n",
    "                    out_ids = self.sample_sequence(\n",
    "                        personality, history, tokenizer, model, args\n",
    "                    )\n",
    "            history.append(out_ids)\n",
    "            history = history[-(2 * args.max_history + 1) :]\n",
    "            out_text = tokenizer.decode(\n",
    "                out_ids, skip_special_tokens=self.args.skip_special_tokens\n",
    "                )\n",
    "            print(\"you->\", raw_text)\n",
    "            print(\"bot->\", out_text)\n",
    "            print(\"--------------------------------\")\n",
    "            # print(history)\n",
    "    def interact_single(self, message, history, personality=None, encode_history=True):\n",
    "        model = self.model\n",
    "        args = self.args\n",
    "        tokenizer = self.tokenizer\n",
    "        process_count = self.args.process_count\n",
    "        if self.args.fp16:\n",
    "            from torch.cuda import amp\n",
    "        self._move_model_to_device()\n",
    "        if not personality:\n",
    "            dataset = get_dataset(\n",
    "                tokenizer,\n",
    "                None,\n",
    "                args.cache_dir,\n",
    "                process_count=process_count,\n",
    "                proxies=self.__dict__.get(\"proxies\", None),\n",
    "                interact=True,\n",
    "            )\n",
    "            personalities = [\n",
    "                dialog[\"personality\"]\n",
    "                for dataset in dataset.values()\n",
    "                for dialog in dataset\n",
    "            ]\n",
    "            personality = random.choice(personalities)\n",
    "        else:\n",
    "            personality = [tokenizer.encode(s.lower()) for s in personality]\n",
    "        if encode_history:\n",
    "            raw_history = history.copy()\n",
    "            raw_history.append(message)\n",
    "            history = [tokenizer.encode(sentence) for sentence in history]\n",
    "        history.append(tokenizer.encode(message))\n",
    "        with torch.no_grad():\n",
    "            if args.fp16:\n",
    "                with amp.autocast():\n",
    "                    out_ids = self.sample_sequence(\n",
    "                        personality, history, tokenizer, model, args\n",
    "                    )\n",
    "            else:\n",
    "                out_ids = self.sample_sequence(\n",
    "                    personality, history, tokenizer, model, args\n",
    "                )\n",
    "        out_text = tokenizer.decode(\n",
    "            out_ids, skip_special_tokens=self.args.skip_special_tokens\n",
    "        )\n",
    "        if encode_history:\n",
    "            raw_history.append(out_text)\n",
    "            history = raw_history\n",
    "        else:\n",
    "            history.append(out_ids)\n",
    "        return out_text, history\n",
    "    \n",
    "    def _threshold(self, x, threshold):\n",
    "        if x >= threshold:\n",
    "            return 1\n",
    "        return 0\n",
    "    def _move_model_to_device(self):\n",
    "        self.model.to(self.device)\n",
    "    def _get_last_metrics(self, metric_values):\n",
    "        return {metric: values[-1] for metric, values in metric_values.items()}\n",
    "    \n",
    "    def _create_training_progress_scores(self, **kwargs):\n",
    "        extra_metrics = {key: [] for key in kwargs}\n",
    "        training_progress_scores = {\n",
    "            \"global_step\": [],\n",
    "            \"language_model_loss\": [],\n",
    "            \"f1_score\": [],\n",
    "            **extra_metrics,\n",
    "        }\n",
    "        return training_progress_scores\n",
    "    \n",
    "    def save_model(self, output_dir=None, model=None, results=None):\n",
    "        if not output_dir:\n",
    "            output_dir = self.args.output_dir\n",
    "        if model and not self.args.no_save:\n",
    "            # Take care of distributed/parallel training\n",
    "            model_to_save = model.module if hasattr(model, \"module\") else model\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            self.tokenizer.save_pretrained(output_dir)\n",
    "            self.save_model_args(output_dir)\n",
    "        if results:\n",
    "            output_eval_file = os.path.join(output_dir, \"eval_results.txt\")\n",
    "            with open(output_eval_file, \"w\") as writer:\n",
    "                for key in sorted(results.keys()):\n",
    "                    writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n",
    "    def add_special_tokens_(self, model, tokenizer):\n",
    "        orig_num_tokens = 32000\n",
    "        num_added_tokens = tokenizer.add_special_tokens(\n",
    "            ATTR_TO_SPECIAL_TOKEN\n",
    "        )  # doesn't add if they are already there\n",
    "        if num_added_tokens > 0:\n",
    "            self.model.resize_token_embeddings(\n",
    "                new_num_tokens=orig_num_tokens + num_added_tokens\n",
    "            )\n",
    "    def build_input_from_segments(self, persona, history, reply, tokenizer, labels=False, with_eos=True):\n",
    "        bos, eos, speaker1, speaker2 = tokenizer.convert_tokens_to_ids(\n",
    "            SPECIAL_TOKENS[:-1]\n",
    "        )\n",
    "        sequence = (\n",
    "            [[bos] + list(chain(*persona))]\n",
    "            + history\n",
    "            + [reply + ([eos] if with_eos else [])]\n",
    "        )\n",
    "        sequence = [sequence[0]] + [\n",
    "            [speaker2 if (len(sequence) - i) % 2 else speaker1] + s\n",
    "            for i, s in enumerate(sequence[1:])\n",
    "        ]\n",
    "        instance = {}\n",
    "        instance[\"input_ids\"] = list(chain(*sequence))\n",
    "        instance[\"token_type_ids\"] = [\n",
    "            speaker2 if i % 2 else speaker1 for i, s in enumerate(sequence) for _ in s\n",
    "        ]\n",
    "        instance[\"mc_token_ids\"] = len(instance[\"input_ids\"]) - 1\n",
    "        instance[\"labels\"] = [-100] * len(instance[\"input_ids\"])\n",
    "        if labels:\n",
    "            instance[\"labels\"] = (\n",
    "                ([-100] * sum(len(s) for s in sequence[:-1]))\n",
    "                + [-100]\n",
    "                + sequence[-1][1:]\n",
    "            )\n",
    "        return instance\n",
    "    \n",
    "    def pad_dataset(self, dataset, padding=0):\n",
    "        max_l = max(len(x) for x in dataset[\"input_ids\"])\n",
    "        for name in PADDED_INPUTS:\n",
    "            dataset[name] = [\n",
    "                x + [padding if name != \"labels\" else -100] * (max_l - len(x))\n",
    "                for x in dataset[name]\n",
    "            ]\n",
    "        return dataset\n",
    "    def top_filtering(self, logits, top_k=0.0, top_p=0.9, threshold=-float(\"Inf\"), filter_value=-float(\"Inf\"), ):\n",
    "        assert (\n",
    "            logits.dim() == 1\n",
    "        )  # Only work for batch size 1 for now - could update but it would obfuscate a bit the code\n",
    "        top_k = min(top_k, logits.size(-1))\n",
    "        if top_k > 0:\n",
    "            # Remove all tokens with a probability less than the last token in the top-k tokens\n",
    "            indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "        if top_p > 0.0:\n",
    "            # Compute cumulative probabilities of sorted tokens\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probabilities = torch.cumsum(\n",
    "                F.softmax(sorted_logits, dim=-1), dim=-1\n",
    "            )\n",
    "            # Remove tokens with cumulative probability above the threshold\n",
    "            sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "            # Shift the indices to the right to keep also the first token above the threshold\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                ..., :-1\n",
    "            ].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            # Back to unsorted indices and set them to -infinity\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[indices_to_remove] = filter_value\n",
    "        indices_to_remove = logits < threshold\n",
    "        logits[indices_to_remove] = filter_value\n",
    "        return logits\n",
    "    def sample_sequence(self, personality, history, tokenizer, model, args, current_output=None):\n",
    "        special_tokens_ids = tokenizer.convert_tokens_to_ids(SPECIAL_TOKENS)\n",
    "        if current_output is None:\n",
    "            current_output = []\n",
    "        for i in range(args.max_length):\n",
    "            instance = self.build_input_from_segments(\n",
    "                personality, history, current_output, tokenizer, with_eos=False\n",
    "            )\n",
    "            input_ids = torch.tensor(\n",
    "                instance[\"input_ids\"], device=self.device\n",
    "            ).unsqueeze(0)\n",
    "            token_type_ids = torch.tensor(\n",
    "                instance[\"token_type_ids\"], device=self.device\n",
    "            ).unsqueeze(0)\n",
    "            logits = model(input_ids, token_type_ids=token_type_ids)\n",
    "            logits = logits[0]\n",
    "            logits = logits[0, -1, :] / args.temperature\n",
    "            logits = self.top_filtering(logits, top_k=args.top_k, top_p=args.top_p)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            prev = (\n",
    "                torch.topk(probs, 1)[1]\n",
    "                if not args.do_sample\n",
    "                else torch.multinomial(probs, 1)\n",
    "            )\n",
    "            if i < args.min_length and prev.item() in special_tokens_ids:\n",
    "                while prev.item() in special_tokens_ids:\n",
    "                    if probs.max().item() == 1:\n",
    "                        warnings.warn(\n",
    "                            \"Warning: model generating special token with probability 1.\"\n",
    "                        )\n",
    "                        break  # avoid infinitely looping over special token\n",
    "                    prev = torch.multinomial(probs, num_samples=1)\n",
    "            if prev.item() in special_tokens_ids:\n",
    "                break\n",
    "            current_output.append(prev.item())\n",
    "        return current_output\n",
    "    def save_model_args(self, output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.args.save(output_dir)\n",
    "    def _load_model_args(self, input_dir):\n",
    "        args = ConvAIArgs()\n",
    "        args.load(input_dir)\n",
    "        return args\n",
    "        \n",
    "    def get_named_parameters(self):\n",
    "        return [n for n, p in self.model.named_parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbf6946-c569-4906-819f-4098f2b10931",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## model_previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9b26636-7e62-4119-b723-6447e820c609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2DoubleHeadsModel were not initialized from the model checkpoint at rinna/japanese-gpt2-small and are newly initialized: ['multiple_choice_head.summary.weight', 'multiple_choice_head.summary.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "train_args = {\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"save_model_every_epoch\": True,\n",
    "    \"output_dir\":\"model_1st\",\n",
    "}\n",
    "model=ConvAIModel(\"rinna/japanese-gpt2-small\", args=train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd18aea5-9298-4d16-95e6-ab7dcd2ccf72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7b0e9b8e5547d28542543074771067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228c919c19794791b6134e09e2874f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1cd3ed508942c99f863047f0ef3123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 5.134336"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31e03587c9a4cadaf12cc1aec9d7d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.984146"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821b3f0e9940465c84b7bf94c5817fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.985030"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23aa7a3a2d54a419c3905e38bdb82a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 4.249205"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372f46384adb403fb798b722319063b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.509876"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee47aa52d8d439c9081119e6cf94c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.294485"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee24209dcb04121a1b803979c2a34cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 3.120820"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef367cfaea149ca80faa72440d61935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.363654"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2a9fb307f474aeeab74cbd353950a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.433721"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77bda7f376a4e7c975d3405af216da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 10 of 10:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.495376"
     ]
    }
   ],
   "source": [
    "model.train_model(\"dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a12906ec-8856-41b8-a45a-9b0ebe881af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'model_1st'\n",
    "output_dir = 'model_3rd'\n",
    "train_args = {\n",
    "    \"num_train_epochs\": 15,\n",
    "    \"save_model_every_epoch\": True,\n",
    "    \"output_dir\":output_dir,\n",
    "    \"tokenizer_dir\":model_dir\n",
    "}\n",
    "\n",
    "model=ConvAIModel(model_dir, args=train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd47f38-0ed2-49f4-a63d-473bb96614aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-11 13:39:23.874911\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45886307746c428fa469d3b1dff6cd55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f39d3096f3e434c906c9929b2e248e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba97dbeff75f4d5d96501a6f0a42847e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.943140"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48060a6624a844079b99fbf5a8e78e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.551828"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0bd49c88e8642aeabb7881559a6f6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.433458"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1620be6fa5d94f3291cb892dab9fad95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 2.220171"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf3a56fa7654f42821ea8f3d60eda66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.991502"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51648c4bbc0e4683ba3ad4ccd66712a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.550896"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9332b5daf0444a048a223b993a52aedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.186088"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13922ab304474866a4d1d0dcf7b7164e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.099318"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5c72dfabc5415b928d2339c64dcaa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 1.116799"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3372c473d24b4ebe471bd876a07649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 10 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.716522"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8189b2754f194a2ab9094afe60040634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 11 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.860496"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d02b534926bc48f9bf4e57215c99baf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 12 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.874881"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319bfcdf311947579bae122c3304aac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 13 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.808736"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e53c6f9ec0044a88b6a8399b0195c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 14 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.502169"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9fb804166484061a2b1aba12722bf4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 15 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.407024"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "model.train_model('dataset.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6775ac7f-ff1b-4f0a-9797-446363b8c7b7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## model_4th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5210352f-55ad-4e78-87af-2c5d80cdef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"model_3rd\"\n",
    "output_dir = \"model_4th\"\n",
    "tensorboard_dir = output_dir+\"/tensorboard_info\"\n",
    "wandb_dir = output_dir+\"/wandb\"\n",
    "arg_4th = {\n",
    "    \"save_steps\":0,\n",
    "    \"dataloader_num_workers\":2,\n",
    "    \"wandb_project\":True,\n",
    "    \"save_model_every_epoch\":True,\n",
    "    \"logging_steps\":100,\n",
    "    \"tensorboard_dir\":tensorboard_dir,\n",
    "    \"output_dir\":output_dir,\n",
    "    \"num_train_epochs\": 15,\n",
    "    \"wandb_dir\":wandb_dir,\n",
    "    # \"tokenizer_dir\":model_dir,\n",
    "\n",
    "}\n",
    "model=ConvAIModel(model_name=model_dir, args=arg_4th,load_tokenizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d2e4c-fd68-448f-a03f-ac45790ec4c2",
   "metadata": {},
   "source": [
    "## model_5th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c855e-b3e2-463b-b3a2-3613c042b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#northern-river-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c008a9af-e8ee-4f64-9975-6a8c36b14504",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"model_4th\"\n",
    "output_dir = \"model_5th\"\n",
    "tensorboard_dir = output_dir+\"/tensorboard_info\"\n",
    "wandb_dir = output_dir+\"/wandb\"\n",
    "arg_5th = {\n",
    "    \"save_steps\":0,\n",
    "    \"dataloader_num_workers\":2,\n",
    "    \"wandb_project\":True,\n",
    "    \"save_model_every_epoch\":True,\n",
    "    \"logging_steps\":100,\n",
    "    \"tensorboard_dir\":tensorboard_dir,\n",
    "    \"output_dir\":output_dir,\n",
    "    \"num_train_epochs\": 15,\n",
    "    \"wandb_dir\":wandb_dir,\n",
    "    \"Lambda_LR\":True,\n",
    "    \"writer\":False,\n",
    "    \"scheduler\":None,\n",
    "    \"learning_rate\":0.00001752\n",
    "\n",
    "}\n",
    "model=ConvAIModel(model_name=model_dir, args=arg_5th,load_tokenizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663f6d9a-aad7-4411-8295-ee0a9057fcde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/conversational_AI/wandb/run-20220812_182634-28rv4xur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlpwolf/notebooks-conversational_AI/runs/28rv4xur\" target=\"_blank\">northern-river-1</a></strong> to <a href=\"https://wandb.ai/nlpwolf/notebooks-conversational_AI\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e507ba58d844899f7be27d2f55ad8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cac613183c47758281e88d2b07ad73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2022-08-12 18:26:52.588765\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a027a6a764924c2eaa44929ae149e734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.263018  2022-08-12 18:39:21.478216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc16f959f37448b5b7daa193b5245e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 2 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.336854  2022-08-12 18:51:51.220143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036a86ea2090416ab8974a9294ace129",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 3 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.197353  2022-08-12 19:04:22.326505\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e1aa9738344c33b7f69a68cd4a0b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 4 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.152743  2022-08-12 19:16:52.872741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91920ea48bf543e8b2cc278d68c29b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 5 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.240815  2022-08-12 19:29:23.132334\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0e8648f12d4c88bc61addccba36a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 6 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.223280  2022-08-12 19:41:53.629203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aae751337c54470b18fd6f84d513ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 7 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.203261  2022-08-12 19:54:27.481184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2c78339c21400bb19a2c503acc592d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 8 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.239662  2022-08-12 20:07:01.354870\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1583328f9949c780325452b889f8de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 9 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.326753  2022-08-12 20:19:35.045864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3d51e336054414b168803dc96aa298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 10 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.302941  2022-08-12 20:32:08.811649\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ed492da8e24a04897ab2226043bfc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 11 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.220468  2022-08-12 20:44:42.000363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14f8687550b483697341314760e1ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 12 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.265029  2022-08-12 20:57:15.280602\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc114d160a334bb286195644879e31fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 13 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.247164  2022-08-12 21:09:49.232557\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbcea9f64309445f88193973062b90ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 14 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.301498  2022-08-12 21:22:22.722058\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6813608108204d8788740bb2bd8f56b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 15 of 15:   0%|          | 0/3785 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loss: 0.276010"
     ]
    }
   ],
   "source": [
    "model.train_model('dataset.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f0b32-f635-40f7-a1f7-a1612c6b3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: learningRate is dropping to zero. fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2328b-cbc2-48e0-88d5-dabb4f8faa72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805b2501-dd38-4ab8-a413-8a46852c13c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7593c1fe-3d72-4e7c-ac7c-a93ec1279b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379c4379-1a6b-497a-ab45-cf069858c3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  あなたの名前はなんですか\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you-> あなたの名前はなんですか\n",
      "bot-> 僕はいつもあなたのバックを拝見してるよ。\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  あなたの名前は？\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you-> あなたの名前は？\n",
      "bot-> 僕はいつもバトーさんのバックを拝見してるよ。 僕はバ\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  こんにちは\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you-> こんにちは\n",
      "bot-> こんにちは 僕の名前は タチコマです。 僕はバトーさんのバック\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  こんにちは\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you-> こんにちは\n",
      "bot-> 僕はタチコマのバックを拝見しています。 僕はタチコマ\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">>>  ありがとう\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you-> ありがとう\n",
      "bot-> 今日はバトーさんのバックを拝見しています。 僕はタチコマ\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# personalityを与えます\n",
    "personality=[\n",
    "\"私の趣味は読書です。\",\n",
    "\"私はガンダムが大好きです。\"\n",
    "]\n",
    "history1 =[\n",
    "                \"こんばんは。お元気ですか？\",\n",
    "                \"はい、元気です。鹿児島での看護師の仕事が忙しいですけど\",\n",
    "                \"お疲れさまです。私は介護福祉士をしています。\"\n",
    "             ]\n",
    "persona = [\n",
    "\"僕の名前はタチコマだよ。\",\n",
    "\"僕はいつも事件解決のお手伝いをしているよ。\",\n",
    "\"僕は公安９課に所属しているよ。\",\n",
    "\"僕はバトーさんの天然オイルが大好物だよ。\"\n",
    "]\n",
    "# 対話を開始する\n",
    "model.interact(personality=persona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8e6367-dc87-4ced-b0b7-31c8154562f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
